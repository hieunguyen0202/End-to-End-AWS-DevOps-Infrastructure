# ğŸ“˜ [AJ3] DevOps Project Documentation

## I. Project Overview

- Project Name: End to End AWS DevOps Infrastructure

- Objective: Automate the build, deployment, and infrastructure provisioning of a Java-based 3-tier web application using CI/CD pipelines and Infrastructure as Code (IaC).

- Key values:
    - Built modular Terraform infrastructure for DEV/UAT/DR, enabling version-controlled, repeatable deployments. Optimized provisioning, reduced manual effort, improved consistency, and ensured high availability across AZs with autoscaling, NAT, ECS, and ALBs.
    - Implemented CI/CD pipeline via GitHub Actions, deploying Java microservices to ECS Fargate with JFrog Artifactory and version tagging, enhancing deployment speed, ensuring release traceability, minimizing human error, and improving delivery consistency across environments.
    - Enhanced infrastructure security by enforcing IAM and security groups via Terraform, minimizing exposure through strict private/public segmentation, detecting code and dependency vulnerabilities with SonarQube and JFrog Xray to reduce attack surface and ensure compliance.
    - Enabled high availability and disaster recovery by configuring multi-AZ RDS (MySQL) with automated backups, cross-region snapshot replication, and structured DR playbooks to meet SLA uptime target of 99.95%.
    - Enforced centralized monitoring and alerting using AWS CloudWatch Alarms, AWS CloudTrail, and VPC Flow Logs to track resource health, container behavior, and suspicious activityâ€”enhancing observability and response readiness across all environments.

- Tech Stack: GitHub Actions, Terraform, Docker, ECS, ECR, SonarCloud, JFrog, RDS (MySQL), Amazon MQ, ElastiCache (Memcached), CloudWatch, CloudFront, ALB, Nginx, Tomcat, Maven.


- AWS Landing Zone

![alt text](EndToEnd-AWSLandingZone.drawio-1.png)

- Architecture Diagram: (AWS-JAVA-3TIER)

![alt text](<End-to-End.drawio (1).svg>)

## II. High-Level Architecture

### ğŸŒ AWS Account Structuring Overview:

```
AWS Organizations
â”‚
â”œâ”€â”€ Root Account
â”‚
â”œâ”€â”€ OU: Sandbox / Dev
â”‚   â””â”€â”€ AWS Account: dev-account
â”‚
â”œâ”€â”€ OU: Non-Prod
â”‚   â””â”€â”€ AWS Account: uat-account

```

- Dev Account: dÃ¹ng bá»Ÿi developers, Ã­t háº¡n cháº¿ (nhÆ°ng váº«n theo IAM, guardrails)
- UAT Account: kiá»ƒm thá»­ trÆ°á»›c khi lÃªn Prod, tÃ¡ch biá»‡t hoÃ n toÃ n khá»i Dev


```
terraform-aws/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”œâ”€â”€ backend.tf (S3 bucket: dev-tf-state)
â”‚   â”‚   â””â”€â”€ provider.tf (assume role to dev AWS account)
â”‚   â””â”€â”€ uat/
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ variables.tf
â”‚       â”œâ”€â”€ backend.tf (bucket: uat-tf-state)
â”‚       â””â”€â”€ provider.tf (assume role to uat AWS account)
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ network-vpc/
â”‚   â””â”€â”€ ecs/
```

### ğŸŒ Backend State per environment:

envs/dev/backend.tf:

```
terraform {
  backend "s3" {
    bucket         = "dev-tf-state"
    key            = "vpc/main.tfstate"
    region         = "ap-southeast-1"
    dynamodb_table = "dev-tf-lock"
    encrypt        = true
  }
}

```

envs/uat/backend.tf:

```
terraform {
  backend "s3" {
    bucket         = "uat-tf-state"
    key            = "vpc/main.tfstate"
    region         = "ap-southeast-1"
    dynamodb_table = "uat-tf-lock"
    encrypt        = true
  }
}

```

### ğŸ” Full Terraform Workflow Step-by-Step

- Developer táº¡o feature branch feature/add-s3-bucket
- Viáº¿t code trong module vÃ  folder envs/dev
- Push lÃªn GitHub -> má»Ÿ Pull Request vÃ o branch dev
- CI Pipeline cháº¡y:
  - terraform init -backend-config=... (dev)
  - terraform plan
  - Upload terraform plan output vÃ o PR comment (CI/CD)

    ```
    - name: Terraform Plan
      id: plan
      run: |
        terraform plan -input=false -no-color > tfplan.txt

    - name: Upload Plan to PR Comment
      uses: juliangruber/terraform-plan-commenter@v1.2.0
      with:
        plan: tfplan.txt
        github_token: ${{ secrets.GITHUB_TOKEN }}
    ```

- Reviewer approve â†’ Merge vÃ o branch dev
- CI cá»§a branch dev cháº¡y terraform apply Tá»° Äá»˜NG lÃªn mÃ´i trÆ°á»ng DEV
- Khi DEV stable â†’ táº¡o PR tá»« dev â†’ main
- CI cháº¡y plan cho mÃ´i trÆ°á»ng UAT (envs/uat)
- Approve & merge â†’ CI branch main cháº¡y terraform apply lÃªn UAT





### ğŸ”„ Blue-Green Deployment Flow (ECS + Jenkins + Terraform)

#### 1. Tag & Build:

Tag source code with a version (e.g., v1.0.0).

Jenkins Build Job builds Docker image and pushes image to ECR with that version tag.

#### 2. Deploy New Version (Terraform):

Update Terraform variables to use the new ECR image version.

Terraform deploys ECS tasks to both green and blue clusters via separate target groups attached to the same ALB.

Application is reachable on two different ALB listeners (e.g., port 80 â†’ blue, port 81 â†’ green).

#### 3. Deploy via Jenkins Job 2 (ECS Deploy):

Jenkins deploy job triggers ECS service update to either blue or green cluster (e.g., deploy new version to green).

Validate new version through separate ALB listener before switching traffic.

#### 4. Switch Traffic:

Jenkins Switch Traffic Job updates ALB listener rules/weights to direct production traffic from blue target group to green.

Zero downtime cutover once verification is complete.

#### 5. Cleanup:

Remove old tasks / cluster (blue or green) via Jenkins Clear Resource Job to free up resources and keep only the active environment.




### ğŸŒ Full Flow Overview:


```
User (Internet)
    â†“
CloudFront CDN (HTTPS, custom domain + caching)
    â†“
S3 Static Website (Frontend React / FE assets)
    â†“
Application Requests
    â†“
ALB (HTTP/HTTPS â€“ public, gateway API routing)
    â†“
ECS Service (Backend API in private subnet)
    â†“
RDS (private DB subnet, port 3306)

```

### ğŸŒ CloudFront + S3 + ALB:


```
User
  â†“
CloudFront
  â”œâ”€â”€ "/"               â†’ S3 static frontend (with OAC / bucket policy restrict to CF)
  â””â”€â”€ "/api/*"          â†’ ALB (my-alb-1234567890.ap-southeast-1.elb.amazonaws.com)
                          â”œâ”€â”€ /api/rdm   â†’ ECS Service rdm
                          â”œâ”€â”€ /api/aegis â†’ ECS Service aegis
                          â””â”€â”€ /api/moai  â†’ ECS Service moai

```



#### ğŸ§± Architecture:

FE (React/Vue/NextJS build files) Ä‘Æ°á»£c build vÃ  upload lÃªn S3.

CloudFront sá»­ dá»¥ng Origin Access Control (OAC) Ä‘á»ƒ truy cáº­p S3.

S3 bucket policy chá»‰ cho phÃ©p access tá»« CloudFront OAC, deny public.

âš™ Terraform Module (vÃ­ dá»¥ tf/modules/s3_fe):
module "frontend_bucket" {
  source       = "./modules/s3_fe"
  bucket_name  = var.bucket_name
  oac_enabled  = true 
}

#### ğŸ§± CI/CD (GitHub Actions) cho FE:

Sau khi npm run build, cháº¡y aws s3 sync build/ s3://my-fe-bucket

Gáº¯n invalidation CloudFront náº¿u cáº§n

```
# .github/workflows/deploy-fe.yaml
jobs:
  deploy_fe:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: npm ci && npm run build
      - uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-1
      - run: aws s3 sync ./build s3://${{ secrets.S3_BUCKET_NAME }}/
      - run: aws cloudfront create-invalidation --distribution-id <CF_ID> --paths "/*"

```


#### ğŸ§± API â€“ ALB â†’ ECS microservices (multi services)

- 1 public ALB + listener 80/443
- Path-based routing
  - /api/rdm â†’ target group RDM â†’ ECS Fargate Task rdm
  - /api/aegis â†’ target group Aegis
  - /api/moai â†’ target group Moai

- Má»—i ECS Service cháº¡y trong private subnet, dÃ¹ng riÃªng security group + IAM Task Role.

```
User
  â†³ ALB
      - Port 80:
         /rdm   -> target_group rdm_blue
         /aegis -> target_group aegis_blue
      - Port 81:
         /rdm   -> target_group rdm_green
         /aegis -> target_group aegis_green


```

- Má»—i ECS Service gá»“m 2 thá»±c thá»ƒ: rdm-blue, rdm-green, aegis-blue, aegis-green.


```
###############################
# Variables (giáº£ sá»­ Ä‘Ã£ define bÃªn ngoÃ i)
###############################
variable "cluster_name" {}
variable "task_execution_role_arn" {}
variable "task_role_arn" {}
variable "image_tag_rdm" {}
variable "image_tag_aegis" {}
variable "vpc_id" {}
variable "subnets" { type = list(string) }
variable "ecs_security_groups" { type = list(string) }

###############################
# ALB
###############################
resource "aws_lb" "app_alb" {
  name               = "api-alb"
  load_balancer_type = "application"
  subnets            = var.subnets
  security_groups    = var.ecs_security_groups
}

# Listener 80 = BLUE
resource "aws_lb_listener" "listener_blue" {
  load_balancer_arn = aws_lb.app_alb.arn
  port              = "80"
  protocol          = "HTTP"

  default_action {
    type             = "fixed-response"
    fixed_response {
      content_type = "text/plain"
      message_body = "Default ALB Response"
      status_code  = "200"
    }
  }
}

# Listener 81 = GREEN
resource "aws_lb_listener" "listener_green" {
  load_balancer_arn = aws_lb.app_alb.arn
  port              = "81"
  protocol          = "HTTP"
  default_action {
    type             = "fixed-response"
    fixed_response {
      content_type = "text/plain"
      message_body = "Green Default"
      status_code  = "200"
    }
  }
}

###############################
# Target Groups (Blue & Green for rdm + aegis)
###############################
resource "aws_lb_target_group" "rdm_blue_tg" {
  name     = "tg-rdm-blue"
  port     = 8080
  protocol = "HTTP"
  vpc_id   = var.vpc_id
  target_type = "ip"
}

resource "aws_lb_target_group" "rdm_green_tg" {
  name     = "tg-rdm-green"
  port     = 8080
  protocol = "HTTP"
  vpc_id   = var.vpc_id
  target_type = "ip"
}

resource "aws_lb_target_group" "aegis_blue_tg" {
  name     = "tg-aegis-blue"
  port     = 8080
  protocol = "HTTP"
  vpc_id   = var.vpc_id
  target_type = "ip"
}

resource "aws_lb_target_group" "aegis_green_tg" {
  name     = "tg-aegis-green"
  port     = 8080
  protocol = "HTTP"
  vpc_id   = var.vpc_id
  target_type = "ip"
}

###############################
# Listener Rules
###############################
# BLUE listener rules
resource "aws_lb_listener_rule" "rdm_rule_blue" {
  listener_arn = aws_lb_listener.listener_blue.arn
  priority     = 10
  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.rdm_blue_tg.arn
  }
  condition {
    path_pattern { values = ["/rdm*"] }
  }
}

resource "aws_lb_listener_rule" "aegis_rule_blue" {
  listener_arn = aws_lb_listener.listener_blue.arn
  priority     = 20
  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.aegis_blue_tg.arn
  }
  condition {
    path_pattern { values = ["/aegis*"] }
  }
}

# GREEN listener rules
resource "aws_lb_listener_rule" "rdm_rule_green" {
  listener_arn = aws_lb_listener.listener_green.arn
  priority     = 10
  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.rdm_green_tg.arn
  }
  condition {
    path_pattern { values = ["/rdm*"] }
  }
}

resource "aws_lb_listener_rule" "aegis_rule_green" {
  listener_arn = aws_lb_listener.listener_green.arn
  priority     = 20
  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.aegis_green_tg.arn
  }
  condition {
    path_pattern { values = ["/aegis*"] }
  }
}

###############################
# Task Definitions
###############################
resource "aws_ecs_task_definition" "rdm_task" {
  family                   = "rdm"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "256"
  memory                   = "512"
  execution_role_arn       = var.task_execution_role_arn
  task_role_arn            = var.task_role_arn

  container_definitions = jsonencode([{
    name  = "rdm"
    image = "430950558682.dkr.ecr.ap-southeast-1.amazonaws.com/rdm:${var.image_tag_rdm}"
    portMappings = [{ containerPort = 8080 }]
    essential = true
  }])
}

resource "aws_ecs_task_definition" "aegis_task" {
  family                   = "aegis"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "256"
  memory                   = "512"
  execution_role_arn       = var.task_execution_role_arn
  task_role_arn            = var.task_role_arn

  container_definitions = jsonencode([{
    name  = "aegis"
    image = "430950558682.dkr.ecr.ap-southeast-1.amazonaws.com/aegis:${var.image_tag_aegis}"
    portMappings = [{ containerPort = 8080 }]
    essential = true
  }])
}

###############################
# ECS Services (Blue & Green) for both Services
###############################
# RDM Services
resource "aws_ecs_service" "rdm_blue_service" {
  name            = "rdm-blue"
  cluster         = var.cluster_name
  task_definition = aws_ecs_task_definition.rdm_task.arn
  desired_count   = 1
  launch_type     = "FARGATE"
  network_configuration {
    subnets         = var.subnets
    security_groups = var.ecs_security_groups
    assign_public_ip = false
  }
  load_balancer {
    target_group_arn = aws_lb_target_group.rdm_blue_tg.arn
    container_name   = "rdm"
    container_port   = 8080
  }
}

resource "aws_ecs_service" "rdm_green_service" {
  name            = "rdm-green"
  cluster         = var.cluster_name
  task_definition = aws_ecs_task_definition.rdm_task.arn
  desired_count   = 0
  launch_type     = "FARGATE"
  network_configuration {
    subnets         = var.subnets
    security_groups = var.ecs_security_groups
    assign_public_ip = false
  }
  load_balancer {
    target_group_arn = aws_lb_target_group.rdm_green_tg.arn
    container_name   = "rdm"
    container_port   = 8080
  }
}

# AEGIS Services
resource "aws_ecs_service" "aegis_blue_service" {
  name            = "aegis-blue"
  cluster         = var.cluster_name
  task_definition = aws_ecs_task_definition.aegis_task.arn
  desired_count   = 1
  launch_type     = "FARGATE"
  network_configuration {
    subnets         = var.subnets
    security_groups = var.ecs_security_groups
    assign_public_ip = false
  }
  load_balancer {
    target_group_arn = aws_lb_target_group.aegis_blue_tg.arn
    container_name   = "aegis"
    container_port   = 8080
  }
}

resource "aws_ecs_service" "aegis_green_service" {
  name            = "aegis-green"
  cluster         = var.cluster_name
  task_definition = aws_ecs_task_definition.aegis_task.arn
  desired_count   = 0
  launch_type     = "FARGATE"
  network_configuration {
    subnets         = var.subnets
    security_groups = var.ecs_security_groups
    assign_public_ip = false
  }
  load_balancer {
    target_group_arn = aws_lb_target_group.aegis_green_tg.arn
    container_name   = "aegis"
    container_port   = 8080
  }
}


```






#### ğŸ§± Blue-Green per Microservice â€“ Architectural Upgrade

- Náº¿u muá»‘n blue-green deployment riÃªng tá»«ng service:

- Option: Weighted Target Groups

- Cho má»—i ECS service, ta táº¡o 2 target group: rdm-blue, rdm-green

- CI/CD deploy image vÃ o ECS green task â†’ test

- Sau test xong, dÃ¹ng aws_lb_listener_rule Ä‘á»ƒ switch weight:
  - 100% traffic â†’ blue
  - 0% â†’ green (initial)

- Sau khi verify: switch 100% â†’ green

- Terraform Cho Blue-Green ECS Service

```
resource "aws_lb_target_group" "rdm_blue" { ... }
resource "aws_lb_target_group" "rdm_green" { ... }
# ECS service rdm-blue, service rdm-green (desired count = 1 or 0)
# Listener Rule Weight:
resource "aws_lb_listener" "rdm_rule" {
   default_action {
      type             = "forward"
      forward {
         target_group {
           arn   = aws_lb_target_group.rdm_blue.arn
           weight = var.weight_blue
         }
         target_group {
           arn   = aws_lb_target_group.rdm_green.arn
           weight = var.weight_green
         }
      }
   }
}

```


#### ğŸŒ  PHáº¦N 1 â€” Cáº¥u trÃºc 2 GitHub Actions riÃªng
.github/workflows/
  â”œâ”€â”€ cicd-rdm.yml
  â””â”€â”€ cicd-aegis.yml

#### ğŸŒ  PHáº¦N 2 â€” VÃ­ dá»¥ GitHub Action cho rdm (kÃ¨m Blue/Green)

```
# .github/workflows/cicd-rdm.yml
name: RDM CICD

on:
  push:
    branches: [ main ]
    paths: ["rdm/**"]        # chá»‰ build khi folder rdm cÃ³ thay Ä‘á»•i

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Build Docker image
      run: |
        docker build -t ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.ap-southeast-1.amazonaws.com/rdm:${{ github.sha }} ./rdm

    - name: Login to Amazon ECR
      uses: aws-actions/amazon-ecr-login@v2

    - name: Push Docker image
      run: |
        docker push ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.ap-southeast-1.amazonaws.com/rdm:${{ github.sha }}

    - name: Blue/Green Deploy (Green == port 81)
      run: |
        # 1) Update ECS service task definition to use new image, deploy to GREEN TG
        aws ecs update-service \
          --cluster rdm-cluster \
          --service rdm-green-svc \
          --task-definition rdm-task:${{ github.sha }}

        # 2) Wait until service stable
        aws ecs wait services-stable \
          --cluster rdm-cluster \
          --services rdm-green-svc

        echo "Green deployed to port 81: now manual or auto test..."

    - name: Swap Traffic if test OK
      if: ${{ github.event.inputs.swap == 'true' }}
      run: |
        # 3) Swap listener rules on ALB: port 80 => Green TG
        aws elbv2 modify-listener \
          --listener-arn arn:aws:elasticloadbalancing:...YOUR-LISTENER-ARN \
          --default-actions Type=forward,TargetGroupArn=arn:aws:elasticloadbalancing:...GREEN-TG-ARN

        # 4) Scale down Blue service to zero
        aws ecs update-service \
          --cluster rdm-cluster \
          --service rdm-blue-svc \
          --desired-count 0

```

- Giáº£i thÃ­ch logic:
  - LuÃ´n deploy image má»›i vÃ o service â€œgreenâ€ (port 81).
  - Kiá»ƒm thá»­ xong náº¿u OK, cháº¡y step Swap Traffic Ä‘á»ƒ modify listener vÃ  scale service blue vá» 0.



#### ğŸ§± CI/CD GitHub Actions cho Backend (ECS):

```
name: Deploy Backend

on:
  push:
    tags:
      - 'v*'   # v1.0.0, v1.0.1

jobs:
  build-and-push:
    ...
    - run: docker build -t $ECR_REPO:$GITHUB_REF_NAME .
    - run: docker push $ECR_REPO:$GITHUB_REF_NAME

  deploy:
    needs: build-and-push
    steps:
      - uses: actions/checkout@v3
      - uses: aws-actions/configure-aws-credentials@v2
      - run: aws ecs update-service --cluster rdm-green --service rdm-green --force-new-deployment
      # hoáº·c dÃ¹ng Terraform apply -var image_tag=${{ github.ref_name }}

```





### ğŸŒ CloudWatch + CloudTrail + CloudConfig Rules

### ğŸ” Secrets Manager rotation policies
   
#### 1ï¸âƒ£ Táº¡o Secret trong Secrets Manager vá»›i Lambda rotation

```
# Secret lÆ°u DB password
resource "aws_secretsmanager_secret" "mysql_secret" {
  name = "mysql-db-password"
  description = "MySQL root password for RDM"
  rotation_lambda_arn = aws_lambda_function.rotate_mysql_secret.arn

  rotation_rules {
    automatically_after_days = 30
  }
}

resource "aws_secretsmanager_secret_version" "mysql_secret_value" {
  secret_id     = aws_secretsmanager_secret.mysql_secret.id
  secret_string = jsonencode({
    username = "admin"
    password = "InitialPassword123"
    host     = "mydb.cluster.amazonaws.com"
  })
}

# Lambda role
resource "aws_iam_role" "lambda_rotation_role" {
  name = "rotation-lambda-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Action = "sts:AssumeRole",
      Effect = "Allow",
      Principal = { Service = "lambda.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_rotation_basic" {
  role       = aws_iam_role.lambda_rotation_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

# Lambda rotation function (vÃ­ dá»¥ MySQL rotation template)
resource "aws_lambda_function" "rotate_mysql_secret" {
  filename         = "lambda_rotation_mysql.zip"  # zip chá»©a code rotation
  function_name    = "RotateMysqlSecret"
  handler          = "index.handler"
  runtime          = "python3.11"
  role             = aws_iam_role.lambda_rotation_role.arn
  timeout          = 60
}

```

ğŸ”¹ LÆ°u Ã½: lambda_rotation_mysql.zip lÃ  AWS cung cáº¥p template rotation function cho MySQL.
#### 2ï¸âƒ£ ECS Task Role cho truy cáº­p secret

```
resource "aws_iam_role" "ecs_task_role" {
  name = "rdm-task-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Action = "sts:AssumeRole",
      Effect = "Allow",
      Principal = { Service = "ecs-tasks.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy" "ecs_task_secret_policy" {
  name = "ecs-task-secret-policy"
  role = aws_iam_role.ecs_task_role.id

  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Action = ["secretsmanager:GetSecretValue"],
      Resource = aws_secretsmanager_secret.mysql_secret.arn
    }]
  })
}

```

#### 3ï¸âƒ£ ECS Task Definition vá»›i Secret

```
resource "aws_ecs_task_definition" "rdm_task" {
  family                   = "rdm-task"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "512"
  memory                   = "1024"
  execution_role_arn       = aws_iam_role.ecs_task_role.arn
  task_role_arn            = aws_iam_role.ecs_task_role.arn

  container_definitions = jsonencode([{
    name      = "rdm-container"
    image     = "123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/rdm:latest"
    essential = true

    environment = [
      {
        name  = "DB_HOST"
        value = "mydb.cluster.amazonaws.com"
      }
    ]

    secrets = [
      {
        name      = "DB_PASSWORD"
        valueFrom = aws_secretsmanager_secret.mysql_secret.arn
      }
    ]

    logConfiguration = {
      logDriver = "awslogs"
      options = {
        awslogs-group         = "/ecs/rdm"
        awslogs-region        = "ap-southeast-1"
        awslogs-stream-prefix = "ecs"
      }
    }
  }])
}

```
ğŸ”¹ LÆ°u Ã½: DB_PASSWORD sáº½ láº¥y trá»±c tiáº¿p tá»« Secrets Manager, khÃ´ng cáº§n hardcode.
#### 4ï¸âƒ£ ECS Service
```
resource "aws_ecs_service" "rdm_service" {
  name            = "rdm-service"
  cluster         = aws_ecs_cluster.rdm_cluster.id
  task_definition = aws_ecs_task_definition.rdm_task.arn
  desired_count   = 2
  launch_type     = "FARGATE"
  network_configuration {
    subnets         = ["subnet-xxxxxxx", "subnet-yyyyyyy"]
    security_groups = ["sg-xxxxxxxx"]
    assign_public_ip = true
  }
}
```
#### 5ï¸âƒ£ Lambda + CloudWatch Event Ä‘á»ƒ redeploy ECS khi secret rotate

```
# Lambda role cho redeploy ECS
resource "aws_iam_role" "ecs_redeploy_lambda_role" {
  name = "ecs-redeploy-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Action = "sts:AssumeRole",
      Effect = "Allow",
      Principal = { Service = "lambda.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy" "ecs_redeploy_lambda_policy" {
  name = "ecs-redeploy-policy"
  role = aws_iam_role.ecs_redeploy_lambda_role.id
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = [
          "ecs:UpdateService",
          "ecs:DescribeServices",
          "ecs:DescribeTaskDefinition"
        ],
        Resource = "*"
      }
    ]
  })
}

# Lambda function redeploy ECS service
resource "aws_lambda_function" "ecs_redeploy" {
  filename         = "redeploy_ecs.zip" # zip chá»©a code Lambda
  function_name    = "ECSRedeployOnSecretRotate"
  handler          = "index.handler"
  runtime          = "python3.11"
  role             = aws_iam_role.ecs_redeploy_lambda_role.arn
  timeout          = 60
}

# CloudWatch Event Rule trigger Lambda khi secret rotated
resource "aws_cloudwatch_event_rule" "secret_rotate_rule" {
  name        = "SecretRotateRule"
  description = "Trigger ECS redeploy when secret rotation happens"
  event_pattern = jsonencode({
    "source": ["aws.secretsmanager"],
    "detail-type": ["AWS API Call via CloudTrail"],
    "detail": {
      "eventName": ["RotateSecret"]
    }
  })
}

resource "aws_cloudwatch_event_target" "secret_rotate_target" {
  rule      = aws_cloudwatch_event_rule.secret_rotate_rule.name
  target_id = "ECSRedeployLambda"
  arn       = aws_lambda_function.ecs_redeploy.arn
}

resource "aws_lambda_permission" "allow_cloudwatch_to_call" {
  statement_id  = "AllowExecutionFromCloudWatch"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.ecs_redeploy.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.secret_rotate_rule.arn
}

```




### ğŸ” Security Group Rules Overview

#### ğŸ“Œ Public NLB Security Group (Public SG)

| **Direction** | **Type** | **Port** | **Source/Destination** | **Purpose**                                      |
|---------------|----------|----------|-------------------------|--------------------------------------------------|
| Inbound       | HTTP     | 80       | 0.0.0.0/0               | Allow traffic from the internet                  |
| Outbound      | HTTP     | 80       | NGINX SG                | Forward request to NGINX reverse proxy           |

---


#### ğŸ“Œ Private NLB Security Group

| **Direction** | **Type** | **Port** | **Source/Destination** | **Purpose**                                      |
|---------------|----------|----------|-------------------------|--------------------------------------------------|
| Inbound       | HTTP     | 8080     | NGINX SG                | Accept traffic from NGINX                        |
| Outbound      | HTTP     | 8080     | ECS Task SG             | Forward to ECS application container             |

---

#### ğŸ“Œ ECS Task Security Group

| **Direction** | **Type** | **Port** | **Source/Destination** | **Purpose**                                      |
|---------------|----------|----------|-------------------------|--------------------------------------------------|
| Inbound       | HTTP     | 8080     | Private NLB SG          | Accept traffic from Private NLB                  |
| Outbound      | All      | All      | 0.0.0.0/0               | Allow internet and DB access                     |

---

#### ğŸ“Œ Private DB Security Group

| **Direction** | **Type**    | **Port** | **Source/Destination** | **Purpose**                                      |
|---------------|-------------|----------|-------------------------|--------------------------------------------------|
| Inbound       | MySQL/Aurora| 3306     | ECS Task SG             | Accept DB queries from ECS task                  |
| Outbound      | All         | All      | 0.0.0.0/0               | Allow system updates, DNS, etc.                  |




## III. High-Level Goals for 99.95% SLA

### Availability & Resilience Strategy

| **Area**              | **Objective**                                                       |
|-----------------------|---------------------------------------------------------------------|
| **High Availability** | Avoid single points of failure (use Multi-AZ & AutoFailover)       |
| **Data Durability**   | Enable automated backups and cross-region replication              |
| **Minimal Downtime**  | Use safe deployment strategies (`create_before_destroy`)           |
| **Disaster Recovery** | Define and test DR runbooks (manual + automated recovery)          |
| **Monitoring & Alerts** | Detect failures fast with alarms and metrics                     |


### Multi-AZ RDS for High Availability

#### â˜‘ï¸ Terraform Task

- Use multi_az = true in your RDS configuration.
- Choose instance class that supports Multi-AZ (e.g., db.m6g.large or above).

```
resource "aws_db_instance" "primary" {
  identifier         = "app-prod-db"
  engine             = "mysql"
  instance_class     = "db.m6g.large"
  multi_az           = true
  ...
}
```

#### â˜‘ï¸ Outcome

- Failover to standby in the other AZ within 1â€“2 minutes.
- AWS manages data replication between AZs.


### Cross-Region Backups

#### â˜‘ï¸ Terraform Task

- Enable `backup_retention_period` and `copy_tags_to_snapshot`
- Use `aws_db_snapshot` and `aws_db_snapshot_copy` for cross-region copies.

```
resource "aws_db_snapshot_copy" "cross_region" {
  source_db_snapshot_identifier = aws_db_snapshot.primary_snapshot.id
  target_db_snapshot_identifier = "db-copy-${timestamp()}"
  kms_key_id                    = aws_kms_key.replica.arn
  source_region                 = var.primary_region
}
```

#### â˜‘ï¸ Outcome
- Backups stored securely in another region for DR readiness.

### Safe Deployments with create_before_destroy

#### â˜‘ï¸ Terraform Task

- Use lifecycle blocks on resources like:
  - Security groups
  - IAM roles
  - Subnets / route tables
  - NLBs
  - EC2 instances

```
resource "aws_security_group" "db_sg" {
  name = "db-sg"

  lifecycle {
    create_before_destroy = true
  }
}

```

#### â˜‘ï¸ Outcome

- Avoids downtime when updating critical infra like SGs or route tables.

### Disaster Recovery (DR) Playbook

#### â˜‘ï¸ Manual or Automated Steps

- Document procedures to restore from cross-region snapshot:
  - Launch RDS from snapshot
  - Update DNS or failover route in Route 53
- Terraform module to quickly stand up infrastructure in secondary region

#### â˜‘ï¸ Automation Tip:
- Use `terraform workspace` or terragrunt to replicate infra to secondary region.


### Monitoring, Alarms, and Failover Automation

- Use CloudWatch Alarms:
  - RDS status checks
  - Freeable memory, CPU, storage thresholds

```
resource "aws_cloudwatch_metric_alarm" "aurora_cpu_high" {
  count               = var.db_mode == "aurora" ? 1 : 0
  alarm_name          = "aurora-cpu-high"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 3
  metric_name         = "CPUUtilization"
  namespace           = "AWS/RDS"
  period              = 300
  statistic           = "Average"
  threshold           = 80
  alarm_description   = "Alarm when Aurora CPU exceeds 80%"
  dimensions = {
    DBClusterIdentifier = aws_rds_cluster.aurora[0].id
  }
  alarm_actions       = [var.sns_topic_arn]  # Send notifications
}

resource "aws_cloudwatch_metric_alarm" "aurora_replica_lag" {
  count               = var.db_mode == "aurora" ? 1 : 0
  alarm_name          = "aurora-replica-lag-high"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 3
  metric_name         = "AuroraReplicaLag"
  namespace           = "AWS/RDS"
  period              = 300
  statistic           = "Average"
  threshold           = 60 # seconds
  alarm_description   = "Alarm when Aurora Replica lag exceeds 60 seconds"
  dimensions = {
    DBClusterIdentifier = aws_rds_cluster.aurora[0].id
  }
  alarm_actions       = [var.sns_topic_arn]
}


```

- CloudWatch Metrics for RDS/Aurora

AWS RDS/Aurora automatically sends key performance and health metrics to CloudWatch, including:

| Metric               | Description                      | Why monitor it?                   |
|----------------------|---------------------------------|---------------------------------|
| CPUUtilization       | % of CPU used                   | Detect high load / bottlenecks  |
| FreeableMemory       | Available RAM (bytes)           | Prevent out-of-memory errors    |
| DatabaseConnections  | Number of active DB connections | Avoid connection saturation     |
| FreeStorageSpace     | Free disk space available       | Avoid running out of disk       |
| ReadIOPS / WriteIOPS | I/O operations per second       | Check for unusual or high I/O load |
| ReadLatency / WriteLatency | Time taken for read/write operations | Detect slow queries or storage issues |
| ReplicaLag           | Lag time of replicas (Aurora only) | Ensure replicas are up-to-date |
| DiskQueueDepth       | Number of pending IO requests   | Identify storage bottlenecks    |
| SwapUsage            | Swap space used                 | Can indicate memory pressure    |


- In a real project, how would you apply CloudWatch for RDS/Aurora?
  - Baseline Metrics & Thresholds: Understand your workload baseline (normal CPU, memory, IOPS). Set thresholds slightly above baseline.

  - Create Alarms for Key Metrics: CPU, storage space, connections, replica lag, latency.

  - Setup Notifications: Use SNS to send alerts to DevOps team via email, Slack, PagerDuty.

  - Automated Actions (optional): Use CloudWatch Event Rules + Lambda to automate scale up/down, or perform remediation.

  - Dashboards: Create CloudWatch Dashboards to visualize trends and performance in one place.

  - Logs: Enable enhanced monitoring & export RDS logs (slow query logs, error logs) to CloudWatch Logs for deeper insights.

  - Integrate with CI/CD: Use alarms as gates to halt deployments if DB performance is degraded.



## IV. Module Documentation

### 1. CI/CD Pipeline (GitHub Actions)

ğŸ”¹ Overview

This pipeline automates build, code scanning, artifact management, image building, security scanning, and deployment to ECS and Nginx.

ğŸ”¸ Workflow Steps:

Backend (Java):

- Checkout source code from GitHub
- Run SonarCloud scan
- Build with Maven
- Push artifacts to JFrog Artifactory
- Build Docker image
- Scan image with Trivy
- Push to AWS ECR
- Deploy image to ECS

Frontend (Static):

- Checkout frontend code
- Build static files (React/Angular/etc.)
- Copy build output to Nginx EC2 server via SSH or deploy with S3 + CloudFront

#### ğŸ“ GitHub Actions Folder Structure:

```
.github/
  workflows/
    aj3-terraform-ci.yml
    aj3-build-cicd.yml

```

#### Documentation for CICD

[ğŸ“˜ How to setup terraform with Ansible ](https://devopsvn.tech/terraform-series/terraform/bai-13-ansible-with-terraform)

[ğŸ“˜ Dependencies Installation ](docs/AJ3-prequisite-setup.md)

[ğŸ“˜ Github Action CICD Terraform Infra](docs/AJ3-CICD-Infra.md)

[ğŸ“˜ Github Action CICD for Build Docker Image](docs/AJ3-CICD-build.md)


### 2. Terraform Infrastructure Modules

ğŸ”¹ Overview

All infrastructure is provisioned as modular Terraform code.

ğŸ”¸ Modules Breakdown

#### ğŸ“¦ Terraform Module Overview

| Module         | Purpose                                                         |
|----------------|-----------------------------------------------------------------|
| `network`          | Create VPC, public/private subnets, IGW, NAT, and route tables, Transit Gateway  |
| `security`     | Define and attach Security Groups  |
| `bastion`      | Launch EC2 instance in public subnet for SSH access |
| `nginx`        | Deploy EC2 instance for static frontend (Nginx server), install sonarqube server          |
| `database`     | Provision RDS (MySQL) with Multi-AZ, Amazon MQ (e.g., ActiveMQ), Memcached via ElastiCache |
| `autoscaling`  | Set up Auto Scaling Group for application servers               |
| `nlb`          | Configure public/private Network Load Balancers                 |
| `iam`          | Create IAM roles and policies for EC2, ECS, and other services  |
| `ecs`          | Create ECS cluster |



#### ğŸ“ Recommended Structure:

```
terraform/
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ network/
â”‚       â””â”€â”€ main.tf        
â”‚   â””â”€â”€ bastion/
â”‚       â””â”€â”€ main.tf        
â”‚   â””â”€â”€ ecs/
â”‚       â””â”€â”€ main.tf        
â”‚   â””â”€â”€ security/
â”‚       â””â”€â”€ main.tf        
â”‚   â””â”€â”€ database/
â”‚       â””â”€â”€ main.tf        
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ backend.tf
â”‚   â”‚   â””â”€â”€ terraform.tfvars
â”‚   â”œâ”€â”€ uat/
â”‚   â”‚   â”œâ”€â”€ backend.tf
â”‚   â”‚   â””â”€â”€ terraform.tfvars
â”‚   â””â”€â”€ pre-prod/
â”‚       â”œâ”€â”€ backend.tf
â”‚       â””â”€â”€ terraform.tfvars
â””â”€â”€ variables.tf           # Common variable definitions

```
#### Terraform design module

![alt text](aj3-terraform-module-design.drawio.png)


#### Create Separate Environments with Workspaces

To use workspaces, eg. DEV environment

```
terraform workspace new dev
terraform workspace select dev
terraform apply -var-file="envs/dev/terraform.tfvars"

```

#### Use Environment-specific Variables

```
env_name       = "dev"
vnet_name      = "dev-vnet"
address_space  = "10.0.0.0/16"
location       = "East US"
resource_group = "rg-dev"

```

#### Use Separate Backends

For state separation across environments, define different backends.

Eg. `envs/dev/backend.tf`

```
terraform {
  backend "s3" {
    bucket         = "my-terraform-state-bucket"   
    key            = "dev/terraform.tfstate"        
    region         = "us-east-1"                    
    dynamodb_table = "terraform-locks"              
    encrypt        = true                           
  }
}

```


### 3. Golden AMI Creation

ğŸ”¹ Overview

Custom AMIs are created to speed up instance launch and enforce consistency.

ğŸ”¸ Global Base AMI

Install: AWS CLI, CloudWatch Agent, SSM Agent

ğŸ”¸ Specialized AMIs:

- Nginx AMI: Install nginx, configure memory metrics
- Tomcat AMI: Install Tomcat, Java 11, systemd setup
- Maven Build AMI: Install Maven, Git, Java 11, preconfigure environment

ğŸ“Œ Optionally use Packer for automation.

### 4. Monitoring & Logging

#### ğŸ” Monitoring & Logging

| Tool           | Purpose                                                                 |
|----------------|-------------------------------------------------------------------------|
| `CloudWatch`   | Log and metric collection for EC2, RDS, and application components       |
| `Cronjob + S3` | Push Tomcat logs to S3 and rotate local logs to save disk space         |
| `Alarms`       | Trigger email alerts on threshold breaches (e.g., DB connections > 100) |
| `SNS`          | Send notifications via Email/SMS when alarms are triggered              |


### 5. Application Lifecycle

#### ğŸ”„ Application Lifecycle

| Phase                   | Tasks                                                                 |
|-------------------------|-----------------------------------------------------------------------|
| `Pre-Deployment`        | Build custom AMIs, configure monitoring agents, SonarCloud & JFrog setup |
| `Infrastructure Deployment` | Run Terraform pipelines to provision AWS infrastructure               |
| `CI/CD Execution`       | Build, scan, and deploy applications using GitHub Actions workflows   |
| `Post-Deployment`       | Set up CloudWatch alerts, validate deployment, and configure log rotation |


## VI. Security Best Practices

- Store secrets in GitHub Secrets or AWS Parameter Store
- Use IAM roles, avoid access keys in pipelines
- Principle of Least Privilege (SGs, IAM, S3 access)
- S3 VPC Endpoint instead of public access
- Enable logging: CloudTrail, VPC Flow Logs

